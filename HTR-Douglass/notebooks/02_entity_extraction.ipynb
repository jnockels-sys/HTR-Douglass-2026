{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009eb1ec-c1d6-4085-974d-dc015dbfcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346c121a-45d9-4654-999c-e35d189940ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 diary entries\n",
      "spaCy model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities: 100%|██████████████████████| 20/20 [00:02<00:00,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diary entries with entities saved: /Users/joenockels/douglass-kg/outputs/entities/HP_ner.json\n",
      "Unique entity list saved: /Users/joenockels/douglass-kg/outputs/entities/HP_unique_entities.json\n",
      "Total unique entities: 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "corrected_json = PROJECT_ROOT / \"data\" / \"processed\" / \"HP_diary_entries.json\"\n",
    "entities_output = PROJECT_ROOT / \"outputs\" / \"entities\" / \"HP_ner.json\"\n",
    "unique_entities_output = PROJECT_ROOT / \"outputs\" / \"entities\" / \"HP_unique_entities.json\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "entities_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Load corrected diary\n",
    "# -----------------------------\n",
    "with open(corrected_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    diary_entries = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(diary_entries)} diary entries\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load spaCy model\n",
    "# -----------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy model loaded\")\n",
    "\n",
    "# -----------------------------\n",
    "# Function to extract entities\n",
    "# -----------------------------\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extract PERSON, ORG, GPE entities\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"EVENT\", \"GPE\"]:\n",
    "            entities.append({\n",
    "                \"text\": ent.text,\n",
    "                \"label\": ent.label_\n",
    "            })\n",
    "    return entities\n",
    "\n",
    "# -----------------------------\n",
    "# Run NER over all entries\n",
    "# -----------------------------\n",
    "for entry in tqdm(diary_entries, desc=\"Extracting entities\"):\n",
    "    entry[\"entities\"] = extract_entities(entry[\"text\"])\n",
    "\n",
    "# Save diary entries with entities\n",
    "with open(entities_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(diary_entries, f, indent=2)\n",
    "\n",
    "print(f\"Diary entries with entities saved: {entities_output}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build a deduplicated entity list\n",
    "# -----------------------------\n",
    "entity_counts = defaultdict(lambda: {\"label\": None, \"count\": 0})\n",
    "\n",
    "for entry in diary_entries:\n",
    "    for ent in entry[\"entities\"]:\n",
    "        key = ent[\"text\"].strip()\n",
    "        entity_counts[key][\"label\"] = ent[\"label\"]\n",
    "        entity_counts[key][\"count\"] += 1\n",
    "\n",
    "# Convert to a list for easy JSON export\n",
    "unique_entities = [{\"name\": k, \"label\": v[\"label\"], \"count\": v[\"count\"]} \n",
    "                   for k, v in entity_counts.items()]\n",
    "\n",
    "# Save deduplicated entities\n",
    "with open(unique_entities_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique_entities, f, indent=2)\n",
    "\n",
    "print(f\"Unique entity list saved: {unique_entities_output}\")\n",
    "print(f\"Total unique entities: {len(unique_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61628eee-115d-4851-9c8b-1c43b469291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fuzzy matching to reduce the amount of manual normalisation / correction, uses character len to find similar\n",
    "# variants and then maps them to a singular canonical name, ready for manual correction and wikidata linking\n",
    "\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbd83c2-5bf6-49b8-9900-495cb9ba006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized entities saved: /Users/joenockels/douglass-kg/outputs/entities/HP_normalized_entities.json\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "unique_entities_file = PROJECT_ROOT / \"outputs\" / \"entities\" / \"HP_unique_entities.json\"\n",
    "normalized_entities_file = PROJECT_ROOT / \"outputs\" / \"entities\" / \"HP_normalized_entities.json\"\n",
    "\n",
    "# Load unique entities\n",
    "with open(unique_entities_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    unique_entities = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# Group similar entity names\n",
    "# -----------------------------\n",
    "threshold = 85  # similarity threshold for fuzzy matching\n",
    "canonical_map = {}  # maps variant -> canonical\n",
    "\n",
    "# Sort entities by length descending (longest first to keep more complete names)\n",
    "unique_entities_sorted = sorted(unique_entities, key=lambda x: len(x[\"name\"]), reverse=True)\n",
    "\n",
    "for entity in unique_entities_sorted:\n",
    "    name = entity[\"name\"]\n",
    "    \n",
    "    # Skip if already mapped\n",
    "    if name in canonical_map:\n",
    "        continue\n",
    "    \n",
    "    # Compare with all other entities\n",
    "    matches = process.extract(\n",
    "        name, \n",
    "        [e[\"name\"] for e in unique_entities_sorted if e[\"name\"] != name], \n",
    "        scorer=fuzz.token_sort_ratio, \n",
    "        score_cutoff=threshold\n",
    "    )\n",
    "    \n",
    "    # Map all variants to this canonical name\n",
    "    for match_name, score, _ in matches:\n",
    "        canonical_map[match_name] = name\n",
    "\n",
    "# Every name maps to itself if no variant found\n",
    "for entity in unique_entities_sorted:\n",
    "    canonical_map.setdefault(entity[\"name\"], entity[\"name\"])\n",
    "\n",
    "# -----------------------------\n",
    "# Build normalized list\n",
    "# -----------------------------\n",
    "normalized_entities = []\n",
    "for entity in unique_entities_sorted:\n",
    "    normalized_entities.append({\n",
    "        \"canonical_name\": canonical_map[entity[\"name\"]],\n",
    "        \"original_name\": entity[\"name\"],\n",
    "        \"label\": entity[\"label\"],\n",
    "        \"count\": entity[\"count\"]\n",
    "    })\n",
    "\n",
    "# Save\n",
    "with open(normalized_entities_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(normalized_entities, f, indent=2)\n",
    "\n",
    "print(f\"Normalized entities saved: {normalized_entities_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84d3072-8e83-403e-87cf-07dab51d3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized entities exported to CSV: /Users/joenockels/douglass-kg/outputs/entities/HP_normalized_entities.csv\n"
     ]
    }
   ],
   "source": [
    "# covert to .csv file for easy data handling, editing \n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "normalized_json = PROJECT_ROOT / \"outputs\" / \"entities\" / \"HP_normalized_entities.json\"\n",
    "csv_path = PROJECT_ROOT / \"outputs\" / \"entities\" / \"HP_normalized_entities.csv\"\n",
    "\n",
    "# Load JSON\n",
    "with open(normalized_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(entities)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Normalized entities exported to CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4606a7-349e-48dd-9c19-b40fc9cc7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract standard_dates to aid the mentions table construction \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "entries_df = pd.DataFrame(diary_entries)\n",
    "\n",
    "# Convert to datetime for safety\n",
    "entries_df[\"standard_date\"] = pd.to_datetime(entries_df[\"standard_date\"])\n",
    "\n",
    "# Sort chronologically\n",
    "entries_df = entries_df.sort_values(\"standard_date\").reset_index(drop=True)\n",
    "\n",
    "entries_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
